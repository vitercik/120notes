
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4. Expectation &#8212; MS&amp;E 120 Course Reader</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter4';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="5. Bibliography" href="bibliography.html" />
    <link rel="prev" title="3. Random variables and their distributions" href="chapter3.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">MS&E 120 Course Reader</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Stanford MS&E 120: Intro to Probability
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter1.html">1. Probability and counting</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter2.html">2. Conditional probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter3.html">3. Random variables and their distributions</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">4. Expectation</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">5. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fchapter4.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/chapter4.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Expectation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-of-expectation">4.1. Definition of expectation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linearity-of-expectation">4.2. Linearity of Expectation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#geometric-and-negative-binomial">4.3. Geometric and negative binomial</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#indicator-random-variables-and-the-fundamental-bridge">4.4. Indicator random variables and the fundamental bridge</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#law-of-the-unconscious-statistician">4.5. Law of the unconscious statistician</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">4.6. Variance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-variance-is-defined-this-way">4.6.1. Why variance is defined this way</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-deviation">4.6.2. Standard deviation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alternate-expression-for-variance">4.6.3. Alternate expression for variance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-properties-of-variance">4.6.4. Important properties of variance</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="expectation">
<h1><span class="section-number">4. </span>Expectation<a class="headerlink" href="#expectation" title="Link to this heading">#</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#definition-of-expectation" id="id5">Definition of expectation</a></p></li>
<li><p><a class="reference internal" href="#linearity-of-expectation" id="id6">Linearity of Expectation</a></p></li>
<li><p><a class="reference internal" href="#geometric-and-negative-binomial" id="id7">Geometric and negative binomial</a></p></li>
<li><p><a class="reference internal" href="#indicator-random-variables-and-the-fundamental-bridge" id="id8">Indicator random variables and the fundamental bridge</a></p></li>
<li><p><a class="reference internal" href="#law-of-the-unconscious-statistician" id="id9">Law of the unconscious statistician</a></p></li>
<li><p><a class="reference internal" href="#variance" id="id10">Variance</a></p>
<ul>
<li><p><a class="reference internal" href="#why-variance-is-defined-this-way" id="id11">Why variance is defined this way</a></p></li>
<li><p><a class="reference internal" href="#standard-deviation" id="id12">Standard deviation</a></p></li>
<li><p><a class="reference internal" href="#alternate-expression-for-variance" id="id13">Alternate expression for variance</a></p></li>
<li><p><a class="reference internal" href="#important-properties-of-variance" id="id14">Important properties of variance</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="definition-of-expectation">
<h2><a class="toc-backref" href="#id5" role="doc-backlink"><span class="section-number">4.1. </span>Definition of expectation</a><a class="headerlink" href="#definition-of-expectation" title="Link to this heading">#</a></h2>
<p>While the full distribution of a RV provides a complete description of its behavior, it can be cumbersome to work with since it involves many probabilities. Instead, it is often more practical to summarize a distribution with just a few key numbers that describe the “average” value of the RV and how much it varies or spreads.</p>
<p>One important summary is the <em>expected value</em>.</p>
<div class="admonition-expected-value admonition">
<p class="admonition-title">Expected value</p>
<p>For a RV <span class="math notranslate nohighlight">\( X \)</span> that takes distinct values <span class="math notranslate nohighlight">\( x_1, x_2, \dots \)</span>, the expected value, denoted <span class="math notranslate nohighlight">\( \mathbb{E}[X] \)</span>, is given by:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{E}[X] = \sum_{i = 1}^{\infty} x_i \mathbb{P}[X = x_i].
\end{equation*}\]</div>
<p>This formula gives a weighted average of the possible values of <span class="math notranslate nohighlight">\( X \)</span>, where the weights are the probabilities associated with each value.</p>
</div>
<p>For example, let <span class="math notranslate nohighlight">\( X \sim \text{Bern}(p) \)</span> be a Bernoulli random variable that takes the value 1 with probability <span class="math notranslate nohighlight">\( p \)</span> and 0 with probability <span class="math notranslate nohighlight">\( 1 - p \)</span>. The expected value of <span class="math notranslate nohighlight">\( X \)</span> is <span class="math notranslate nohighlight">\(\mathbb{E}[X] = 0 \cdot \mathbb{P}[X = 0] + 1 \cdot \mathbb{P}[X = 1] = 1 \cdot p = p.\)</span>
This means that the expected value of a Bernoulli random variable is simply <span class="math notranslate nohighlight">\( p \)</span>, the probability of success.</p>
<p>Below, we illustrate the <span class="math notranslate nohighlight">\(\text{Bern}(p)\)</span> PMF for several choices of <span class="math notranslate nohighlight">\(p\)</span>, together with the expected value.</p>
<figure class="align-center" id="bern25">
<a class="reference internal image-reference" href="_images/bern25.png"><img alt="_images/bern25.png" src="_images/bern25.png" style="width: 225px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.1 </span><span class="caption-text">PMF of the <span class="math notranslate nohighlight">\(\text{Bern}(0.5)\)</span> distribution, with the expected value marked by a star.</span><a class="headerlink" href="#bern25" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="bern50">
<a class="reference internal image-reference" href="_images/bern50.png"><img alt="_images/bern50.png" src="_images/bern50.png" style="width: 225px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.2 </span><span class="caption-text">PMF of the <span class="math notranslate nohighlight">\(\text{Bern}(0.5)\)</span> distribution, with the expected value marked by a star.</span><a class="headerlink" href="#bern50" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="bern75">
<a class="reference internal image-reference" href="_images/bern75.png"><img alt="_images/bern75.png" src="_images/bern75.png" style="width: 225px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.3 </span><span class="caption-text">PMF of the <span class="math notranslate nohighlight">\(\text{Bern}(0.75)\)</span> distribution, with the expected value marked by a star.</span><a class="headerlink" href="#bern75" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>As another example, let <span class="math notranslate nohighlight">\(X\)</span> be the number of touchdown passes thrown by the Packers quarterback in a game between the Bears and the Packers, with its PMF as follows:</p>
<div class="pst-scrollable-table-container"><table class="table" id="touchdown">
<caption><span class="caption-number">Table 4.1 </span><span class="caption-text">PMF of the number of touchdown passes thrown by Packers quarterback.</span><a class="headerlink" href="#touchdown" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(x\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\( p_X(x) = \mathbb{P}[X = x] \)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0.10</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>0.25</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>0.25</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>0.25</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>0.10</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>0.05</p></td>
</tr>
</tbody>
</table>
</div>
<p>Then <span class="math notranslate nohighlight">\(\mathbb{E}[X] = 0 \cdot 0.1 + 1 \cdot 0.25 + 2 \cdot 0.25 + 3 \cdot 0.25 + 4 \cdot 0.1 + 5 \cdot 0.05 = 2.15\)</span>.</p>
<p>We illustrate the PMF and expected value of this random variable below.</p>
<figure class="align-center" id="football-exp">
<a class="reference internal image-reference" href="_images/football_exp.png"><img alt="_images/football_exp.png" src="_images/football_exp.png" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.4 </span><span class="caption-text">Illustration of the PMF and expected number of touchdown passes thrown by the Packers quarterback.</span><a class="headerlink" href="#football-exp" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="tip admonition">
<p class="admonition-title">Example: Investment returns.</p>
<p>Suppose an investment has a 50% chance of gaining 20% and a 50% chance of losing 10%. To calculate the expected return, we multiply the probability of each outcome by the corresponding return and sum the results, so the expected return is <span class="math notranslate nohighlight">\(0.5 \cdot 0.2 + 0.5 \cdot (-0.1) = 0.05.\)</span>
In other words, the expected return of this investment is 5%.</p>
<p>Meanwhile, suppose the investment has a 50% chance of gaining 200% and a 50% chance of losing 190%. The expected return is <span class="math notranslate nohighlight">\(0.5 \cdot 2.0 + 0.5 \cdot (-1.9) = 0.05.\)</span> Even though the potential gains and losses are much larger in this case, the expected return is still 5% (however, the <em>variance</em> is different, a topic we will return to later in this chapter).</p>
</div>
</section>
<section id="linearity-of-expectation">
<h2><a class="toc-backref" href="#id6" role="doc-backlink"><span class="section-number">4.2. </span>Linearity of Expectation</a><a class="headerlink" href="#linearity-of-expectation" title="Link to this heading">#</a></h2>
<p>One of the fundamental properties of expectation is <em>linearity:</em> the expected value of a sum of random variables equals the sum of their individual expected values, regardless of whether these random variables are independent.</p>
<div class="admonition-linearity-of-expectation admonition">
<p class="admonition-title">Linearity of expectation</p>
<ol class="arabic simple">
<li><p><strong>Sum of random variables</strong>: The expectation of the sum of two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> can be expressed as: <span class="math notranslate nohighlight">\(\mathbb{E}[X + Y] = \mathbb{E}[X] + \mathbb{E}[Y].\)</span> Notably, this property holds even if <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are not independent.</p></li>
<li><p><strong>Scaling of random variables</strong>: For a random variable <span class="math notranslate nohighlight">\(X\)</span> and any constant <span class="math notranslate nohighlight">\(c \in \mathbb{R}\)</span>, the expectation scales linearly: <span class="math notranslate nohighlight">\(\mathbb{E}[cX] = c \mathbb{E}[X].\)</span></p></li>
</ol>
</div>
<p>These properties allow us to decompose complex random variables into simpler parts for easier computation.</p>
<p>For example, let <span class="math notranslate nohighlight">\(X \sim \text{Bin}(n, p)\)</span> be a binomial RV. We know that <span class="math notranslate nohighlight">\(X = I_1 + I_2 + \cdots + I_n\)</span>, where each <span class="math notranslate nohighlight">\(I_j \sim \text{Bern}(p)\)</span>. Since <span class="math notranslate nohighlight">\(\mathbb{E}[I_j] = p\)</span>, we can conclude that <span class="math notranslate nohighlight">\(\mathbb{E}[X] =  \mathbb{E}[I_1 + I_2 + \cdots + I_n] = \mathbb{E}[I_1] + \cdots + \mathbb{E}[I_n] = np.\)</span></p>
<p>As another example, suppose we have a bag with <span class="math notranslate nohighlight">\(w\)</span> white marbles and <span class="math notranslate nohighlight">\(b\)</span> black marbles. Let <span class="math notranslate nohighlight">\(X \sim \text{HGeom}(w, b, n)\)</span> represent the number of white marbles when we draw <span class="math notranslate nohighlight">\(n\)</span> marbles out of the bag randomly without replacement. Define the RV <span class="math notranslate nohighlight">\(I_j\)</span> such that <span class="math notranslate nohighlight">\(I_j = 1\)</span> if the <span class="math notranslate nohighlight">\(j\)</span>-th marble in the sample is white, and <span class="math notranslate nohighlight">\(I_j = 0\)</span> otherwise. Then, the RV <span class="math notranslate nohighlight">\(X\)</span> can be expressed as <span class="math notranslate nohighlight">\(X = I_1 + I_2 + \cdots + I_n.\)</span> The RVs <span class="math notranslate nohighlight">\(I_1, \dots, I_n\)</span> are not independent, but linearity of expectation still applies. For each indicator variable <span class="math notranslate nohighlight">\(I_j\)</span>,</p>
<div class="math notranslate nohighlight" id="equation-hyp-exp">
<span class="eqno">(4.1)<a class="headerlink" href="#equation-hyp-exp" title="Link to this equation">#</a></span>\[\mathbb{E}[I_j] = 1 \cdot \mathbb{P}[I_j = 1] + 0 \cdot \mathbb{P}[I_j = 0] = \mathbb{P}[\text{$j$-th marble is white}].\]</div>
<p>To calculate this probability, let’s start with the first marble. Since there are <span class="math notranslate nohighlight">\(w\)</span> white marbles and <span class="math notranslate nohighlight">\(w+ b\)</span> total marbles, <span class="math notranslate nohighlight">\(\mathbb{P}[1^{st} \text{ marble is white}] = \frac{w}{w+b}\)</span>. Next,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb{P}[2^{nd} \text{ marble is white}] = \, &amp;\mathbb{P}[2^{nd} \text{ marble is white} \mid 1^{st} \text{ marble is white}]\mathbb{P}[1^{st} \text{ marble is white}]\\
			+\, &amp;\mathbb{P}[2^{nd} \text{ marble is white} \mid 1^{st} \text{ marble is black}]\mathbb{P}[1^{st} \text{ marble is black}].
\end{align*}\]</div>
<p>If we know the first marble is white, then there are <span class="math notranslate nohighlight">\(w-1\)</span> white marbles out of the remaining <span class="math notranslate nohighlight">\(w+b-1\)</span> marbles. Therefore, <span class="math notranslate nohighlight">\(\mathbb{P}[2^{nd} \text{ marble is white} \mid 1^{st} \text{ marble is white}] = \frac{w-1}{w+b-1}\)</span>. Meanwhile, <span class="math notranslate nohighlight">\(\mathbb{P}[2^{nd} \text{ marble is white} \mid 1^{st} \text{ marble is black}] = \frac{w}{w+b-1}\)</span>. Therefore,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{P}[2^{nd} \text{ marble is white}] = \frac{w-1}{w+b-1} \cdot \frac{w}{w+b} + \frac{w}{w+b-1}\left(1 - \frac{w}{w+b}\right) = \frac{w}{w+b}.
\end{equation*}\]</div>
<p>Returning to <a class="reference internal" href="#equation-hyp-exp">(4.1)</a>, we have that <span class="math notranslate nohighlight">\(\mathbb{E}[I_j] =  \mathbb{P}[j^{th} \text{ marble is white}] = \frac{w}{w+b}\)</span>. In other words, each marble we draw is equally likely to be white when considered in isolation (though not when conditioned on the previous draws), a phenomenon known as <em>symmetry</em>. In the end, we have that <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \mathbb{E}[I_1 + \dots + I_n] = \mathbb{E}[I_1] + \cdots + \mathbb{E}[I_n] =  \frac{nw}{w+b}\)</span>.</p>
<p>We note that the events that the <span class="math notranslate nohighlight">\(1^{st}\)</span> and <span class="math notranslate nohighlight">\(2^{nd}\)</span> marbles are white are not independent: <span class="math notranslate nohighlight">\(\mathbb{P}[2^{nd} \text{ marble is white} \mid 1^{st} \text{ marble is white}] = \frac{w-1}{w+b-1} &lt; \mathbb{P}[2^{nd} \text{ marble is white}]  = \frac{w}{w+b}\)</span>. This intuitively makes sense: once you know the first marble is white, there are fewer white marbles to choose from.</p>
</section>
<section id="geometric-and-negative-binomial">
<h2><a class="toc-backref" href="#id7" role="doc-backlink"><span class="section-number">4.3. </span>Geometric and negative binomial</a><a class="headerlink" href="#geometric-and-negative-binomial" title="Link to this heading">#</a></h2>
<div class="admonition-geometric-distribution admonition">
<p class="admonition-title">Geometric distribution</p>
<p>The geometric distribution arises from a sequence of independent Bernoulli trials, where each trial results in a success with probability <span class="math notranslate nohighlight">\( p \)</span> and a failure with probability <span class="math notranslate nohighlight">\( q = 1 - p \)</span>. In this context, let the random variable <span class="math notranslate nohighlight">\( X \)</span> represent the number of failures that occur before the first success. We say that <span class="math notranslate nohighlight">\( X \)</span> follows a <em>geometric distribution with parameter <span class="math notranslate nohighlight">\( p \)</span></em>, denoted as <span class="math notranslate nohighlight">\( X \sim \text{Geom}(p) \)</span>.</p>
<p>The probability mass function <span class="math notranslate nohighlight">\( p_X(k) \)</span> of a geometric random variable is <span class="math notranslate nohighlight">\(\mathbb{P}[X = k] = p_X(k) = q^k p\)</span> for <span class="math notranslate nohighlight">\(k = 0, 1, 2, \dots\)</span>. This expression reflects the probability of having <span class="math notranslate nohighlight">\( k \)</span> consecutive failures (each occurring with probability <span class="math notranslate nohighlight">\( q \)</span>) followed by a single success (occurring with probability <span class="math notranslate nohighlight">\( p \)</span>). For example, if we observe the sequence “FFFFFS,” where five failures are followed by one success, the probability of this specific sequence occurring is given by <span class="math notranslate nohighlight">\( q^5 p \)</span>.</p>
</div>
<p>The expectation <span class="math notranslate nohighlight">\( \mathbb{E}[X] \)</span> for a geometric random variable can be derived by summing over all possible values of <span class="math notranslate nohighlight">\( k \)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{E}[X] = \sum_{k = 0}^{\infty} k \cdot \mathbb{P}[X = k] = p \sum_{k = 0}^{\infty} k q^k = \frac{1 - p}{p}.
\end{equation*}\]</div>
<p>The following distribution is closely related to the geometric distribution.</p>
<div class="admonition-first-success-distribution admonition">
<p class="admonition-title">First success distribution</p>
<p>Suppose we perform a sequence of independent Bernoulli trials, where each trial results in a success with probability <span class="math notranslate nohighlight">\( p \)</span> and a failure with probability <span class="math notranslate nohighlight">\( q = 1 - p \)</span>. Let <span class="math notranslate nohighlight">\( Y \)</span> be the total number of trials required to achieve the first successful outcome, <em>including the successful trial itself</em>. Then <span class="math notranslate nohighlight">\(Y\)</span> follows the <em>first success distribution with parameter <span class="math notranslate nohighlight">\(p\)</span></em>, denoted <span class="math notranslate nohighlight">\( Y \sim \text{FS}(p) \)</span>.</p>
</div>
<p>The key relationship between <span class="math notranslate nohighlight">\( Y \)</span> and the geometric distribution is that <span class="math notranslate nohighlight">\( Y \)</span> counts the total number of trials, while the geometric random variable <span class="math notranslate nohighlight">\( X \)</span> only counts the number of failures before the first success. Mathematically, <span class="math notranslate nohighlight">\(Y = X+1\)</span>, where <span class="math notranslate nohighlight">\( X \sim \text{Geom}(p) \)</span>. Since <span class="math notranslate nohighlight">\( Y \)</span> simply adds one successful trial to the count of failures, the expected value of <span class="math notranslate nohighlight">\( Y \)</span> can be derived using the linearity of expectation:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{E}[Y] = \mathbb{E}[X + 1] = \mathbb{E}[X] + 1 = \frac{1 - p}{p} + 1 = \frac{1}{p}.
\end{equation*}\]</div>
<p>In other words, on average, the number of trials required to achieve the first success (including the successful trial itself) is <span class="math notranslate nohighlight">\( \frac{1}{p} \)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<p>Imagine a couple who decides to continue having children until they have at least one boy and one girl. Each child is either a boy or a girl, and each outcome is independent with an equal probability of 50%. The couple never has twins, so each child is born individually.</p>
<p><strong>Question:</strong> What is the expected number of children they’ll have?</p>
<p>For example, if the sequence of births is G-G-G-B, they would have four children in total. If the sequence is B-B-G, they would have three children in total.</p>
<ol class="arabic">
<li><p><strong>Step 1: Define the Random Variables</strong></p>
<p>Let <span class="math notranslate nohighlight">\( X \)</span> denote the number of children needed, starting from the second child, to get a gender different from the firstborn. Essentially, <span class="math notranslate nohighlight">\( X \)</span> is the number of additional children required after the first child to achieve a child of the opposite gender. For example, if the birth order is G-G-G-B, then <span class="math notranslate nohighlight">\(X = 3\)</span> and if the birth order is B-B-G, then <span class="math notranslate nohighlight">\(X = 2\)</span>.</p>
</li>
<li><p><strong>Step 2: Define the Goal</strong></p>
<p>The goal is to compute the expected number of children the couple will have, which can be expressed as <span class="math notranslate nohighlight">\(\mathbb{E}[\# \text{children}] = \mathbb{E}[1 + X] = 1 + \mathbb{E}[X]\)</span>. Here, the “1” represents the firstborn, and <span class="math notranslate nohighlight">\( X \)</span> represents the additional children needed.</p>
</li>
<li><p><strong>Step 3: Identify the Distribution of <span class="math notranslate nohighlight">\( X \)</span></strong></p>
<p>Notice that <span class="math notranslate nohighlight">\( X \)</span> represents the number of children until the couple has a child of the opposite gender from the firstborn. This is equivalent to the distribution <span class="math notranslate nohighlight">\( \text{FS}(\frac{1}{2}) \)</span>, which describes the number of trials until the first success, given a success probability of <span class="math notranslate nohighlight">\( \frac{1}{2} \)</span>. The expected value of <span class="math notranslate nohighlight">\( X \)</span> is <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \frac{1}{\frac{1}{2}} = 2\)</span>.</p>
</li>
<li><p><strong>Step 4: Calculate the Final Answer</strong></p>
<p>The total expected number of children the couple will have is: <span class="math notranslate nohighlight">\(\mathbb{E}[\# \text{children}] = 1 + \mathbb{E}[X] = 1 + 2 = 3.\)</span></p>
</li>
</ol>
</div>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<p>This is Example 4.3.12 from <span id="id1">[<a class="reference internal" href="bibliography.html#id2" title="Joseph K Blitzstein and Jessica Hwang. Introduction to probability. Chapman and Hall/CRC, 2019.">BH19</a>]</span>.</p>
<p>This is a classic problem called the coupon collector problem. Imagine that there are <span class="math notranslate nohighlight">\( n \)</span> different types of toys that you are trying to collect. Each time you acquire a toy, it is equally likely to be any one of the <span class="math notranslate nohighlight">\( n \)</span> types, regardless of the toys you have collected so far. The goal is to determine the expected number of toys you need to collect in order to have at least one of each type.</p>
<p><strong>Question:</strong> What is the expected number of toys needed to complete the set?</p>
<ol class="arabic">
<li><p><strong>Step 1: Define the Random Variables</strong></p>
<p>Let <span class="math notranslate nohighlight">\( N \)</span> denote the total number of toys needed to collect all <span class="math notranslate nohighlight">\( n \)</span> different types. This can be expressed as the sum of individual random variables <span class="math notranslate nohighlight">\( N_1, N_2, \ldots, N_n \)</span>, where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( N_1 \)</span> is the number of toys needed to collect the first type that you haven’t seen before. Clearly, <span class="math notranslate nohighlight">\( N_1 = 1 \)</span> because the first toy you collect will always be a new type.</p></li>
<li><p><span class="math notranslate nohighlight">\( N_2 \)</span> is the number of additional toys needed to collect a second type that you haven’t seen before.</p></li>
<li><p><span class="math notranslate nohighlight">\( N_j \)</span> is the number of additional toys needed to collect the <span class="math notranslate nohighlight">\( j \)</span>-th type that you haven’t seen before.</p></li>
</ul>
<p>Thus, <span class="math notranslate nohighlight">\(N = N_1 + N_2 + \cdots + N_n.\)</span></p>
</li>
<li><p><strong>Step 2: Identify the Distribution of <span class="math notranslate nohighlight">\( N_j \)</span></strong></p>
<p>Each <span class="math notranslate nohighlight">\( N_j \)</span> follows the first success distribution, where the probability of success depends on how many types are yet to be collected.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( N_2 \sim \text{FS}\left(\frac{n-1}{n}\right) \)</span>, as you are looking for a toy of a new type among the <span class="math notranslate nohighlight">\( n \)</span> available types, given that you’ve already collected one type.</p></li>
<li><p>More generally, <span class="math notranslate nohighlight">\( N_j \)</span> follows <span class="math notranslate nohighlight">\( \text{FS}\left(\frac{n-j+1}{n}\right) \)</span> because you are searching for a new type of toy among the remaining <span class="math notranslate nohighlight">\( n - (j - 1) \)</span> types.</p></li>
</ul>
</li>
<li><p><strong>Step 3: Compute the Final Answer</strong>
The expected total number of toys <span class="math notranslate nohighlight">\( N \)</span> is given by the sum of the expected values of each <span class="math notranslate nohighlight">\( N_j \)</span>. Therefore,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{E}[N] = \mathbb{E}[N_1] + \mathbb{E}[N_2] + \cdots + \mathbb{E}[N_n] = 1 + \frac{n}{n-1} + \frac{n}{n-2} + \cdots + n \approx n \log n.
\end{equation*}\]</div>
</li>
</ol>
</div>
<p>We will introduce one more distribution that is closely related to the geometric distribution.</p>
<div class="admonition-negative-binomial-distribution admonition">
<p class="admonition-title">Negative binomial distribution</p>
<p>Suppose we perform a sequence of independent Bernoulli trials, where each trial results in a success with probability <span class="math notranslate nohighlight">\( p \)</span>. Let <span class="math notranslate nohighlight">\(X\)</span> be the number of failures before <span class="math notranslate nohighlight">\(r^{th}\)</span> success. Then we say that <span class="math notranslate nohighlight">\(X\)</span> follows the *negative binomial distribution with parameters <span class="math notranslate nohighlight">\((r,p)\)</span>, denoted <span class="math notranslate nohighlight">\(X \sim \text{NBin}(r,p)\)</span>.</p>
</div>
<p>To determine the PMF of <span class="math notranslate nohighlight">\(X\)</span>, we will start with an example, where <span class="math notranslate nohighlight">\(X \sim \text{NBin}(5, p)\)</span>, which is the number of failures before the fifth success. Under the following sequences (with 1 representing success and 0 representing failure), there are three failures before the fifth success: 11010011, or 11010101, or … Indeed, to represent three failures before the fifth success, the last digit in these sequences <strong>must</strong> be a 1, and must be preceded by three 0s (the three failures) and four 1s (the four other successes), arranged in any order. There are <span class="math notranslate nohighlight">\({3 + 4 \choose 3}\)</span> different sequences of this form: imagine drawing seven fill-in-the-blank slots, three of which we must fill in with 0s, and the rest with 1s. Each sequence has a probability of <span class="math notranslate nohighlight">\(p^5(1-p)^3\)</span> (for the five success and the three failures). Therefore</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}

\mathbb{P}[X = 3] = {3 + 4 \choose 3}p^5(1-p)^3.

\end{equation*}\]</div>
<p>By the exact same logic, for <span class="math notranslate nohighlight">\(X \sim \text{NBin}(r,p)\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}

\mathbb{P}[X = n] = {n + r-1 \choose n}p^r(1-p)^n

\end{equation*}\]</div>
<p>for <span class="math notranslate nohighlight">\(n = 0, 1, 2, \dots\)</span>.
To calculate the expectation of <span class="math notranslate nohighlight">\(X\)</span>, we can write it as <span class="math notranslate nohighlight">\(X = X_1 + \cdots + X_r\)</span> where <span class="math notranslate nohighlight">\(X_1\)</span> is the number of failures until the <span class="math notranslate nohighlight">\(1^{st}\)</span> success, <span class="math notranslate nohighlight">\(X_2\)</span> is the number of failures between <span class="math notranslate nohighlight">\(1^{st}\)</span> and second <span class="math notranslate nohighlight">\(2^{nd}\)</span> successes, and so on. We can see that <span class="math notranslate nohighlight">\(X_i \sim \text{Geom}(p)\)</span>, so by linearity of expectation, <span class="math notranslate nohighlight">\(\mathbb{E}[X] = r \cdot \frac{1-p}{p}\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<p>This season, suppose Arsenal wins each soccer game independently with a probability of 0.7.</p>
<p><strong>1. Number of Games Before a Win.</strong> Let <span class="math notranslate nohighlight">\( X \)</span> be the number of games Arsenal plays before winning their first game. Since each game is independent and has a fixed probability of success (a win) of 0.7, <span class="math notranslate nohighlight">\(X \sim \text{Geom}(0.7).\)</span></p>
<p><strong>2. Number of Losses Before Winning Five Games.</strong> Next, let <span class="math notranslate nohighlight">\( Y \)</span> represent the number of games Arsenal loses before achieving five wins. Then <span class="math notranslate nohighlight">\(Y \sim \text{NBin}(5, 0.7).\)</span></p>
</div>
</section>
<section id="indicator-random-variables-and-the-fundamental-bridge">
<h2><a class="toc-backref" href="#id8" role="doc-backlink"><span class="section-number">4.4. </span>Indicator random variables and the fundamental bridge</a><a class="headerlink" href="#indicator-random-variables-and-the-fundamental-bridge" title="Link to this heading">#</a></h2>
<p>An indicator random variable is a simple variable that indicates whether a specific event has occurred or not.</p>
<div class="admonition-indicator-random-variable admonition">
<p class="admonition-title">Indicator random variable</p>
<p>Given an event <span class="math notranslate nohighlight">\(A\)</span>, define the RV <span class="math notranslate nohighlight">\( I_A \)</span> is as follows:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
I_A =
\begin{cases}
1 &amp; \text{if event } A \text{ occurs} \\
0 &amp; \text{otherwise}.
\end{cases}
\end{equation*}\]</div>
</div>
<p>The fundamental relationship between the probability and expectation of an indicator RV is captured by the following formula:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{E}[I_A] = 1 \cdot \mathbb{P}[I_A = 1] + 0 \cdot \mathbb{P}[I_A = 0] = \mathbb{P}[I_A = 1] = \mathbb{P}[A].
\end{equation*}\]</div>
<p>This equation shows that the expected value of an indicator random variable <span class="math notranslate nohighlight">\( I_A \)</span> is equal to the probability of the event <span class="math notranslate nohighlight">\( A \)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<p>Suppose a university is deciding whether to test all 100 people in a dormitory for COVID-19. Before conducting individual tests, they will test the sewage. If the sewage sample tests positive, they will test all 100 people in the dorm. Let’s model this situation using indicator random variables:</p>
<ul class="simple">
<li><p>Each person in the dorm is independently negative with a probability of 0.9.</p></li>
<li><p>Let <span class="math notranslate nohighlight">\( X \)</span> represent the total number of tests conducted, including the sewage test.</p></li>
<li><p>Let <span class="math notranslate nohighlight">\( A \)</span> denote the event that the sewage test is positive, with indicator RV <span class="math notranslate nohighlight">\(I_A\)</span>.</p></li>
</ul>
<p>The total number of tests <span class="math notranslate nohighlight">\( X \)</span> is <span class="math notranslate nohighlight">\(X = 1 + 100 \cdot I_A\)</span>. Here, the “1” represents the initial sewage test, and the term <span class="math notranslate nohighlight">\( 100 \cdot I_A \)</span> represents the additional 100 individual tests if the sewage test is positive. To find the expected number of tests <span class="math notranslate nohighlight">\( \mathbb{E}[X] \)</span>, we first need to determine <span class="math notranslate nohighlight">\( \mathbb{E}[I_A] \)</span>. The probability of <span class="math notranslate nohighlight">\( A \)</span>, which is the probability that at least one person in the dorm has COVID-19, is given by <span class="math notranslate nohighlight">\(\mathbb{P}[A] = 1 - \mathbb{P}[\text{no one has COVID}] = 1 - 0.9^{100}.\)</span> Therefore, the expected value of <span class="math notranslate nohighlight">\( I_A \)</span> is <span class="math notranslate nohighlight">\(\mathbb{E}[I_A] = \mathbb{P}[A] = 1 - 0.9^{100}\)</span>.</p>
<p>Now, we can calculate the expected number of tests:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{E}[X] = \mathbb{E}[1 + 100 \cdot I_A] = 1 + 100 \mathbb{E}[I_A] = 1 + 100(1 - 0.9^{100}).
\end{equation*}\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<p>This is Example 4.4.5 from <span id="id2">[<a class="reference internal" href="bibliography.html#id2" title="Joseph K Blitzstein and Jessica Hwang. Introduction to probability. Chapman and Hall/CRC, 2019.">BH19</a>]</span>.</p>
<p>In a room with <span class="math notranslate nohighlight">\( n \)</span> people, each person’s birthday is equally likely to fall on any of the 365 days of the year. The goal is to calculate the expected number of birthday matches, denoted as <span class="math notranslate nohighlight">\( Y \)</span>, where a birthday match occurs if two people share the same birthday.</p>
<p><strong>Step 1: Define the random variables.</strong> We begin by labeling the people in the room as <span class="math notranslate nohighlight">\(1, 2, 3, \dots, n \)</span> and ordering the <span class="math notranslate nohighlight">\( \binom{n}{2} \)</span> pairs of people. Let <span class="math notranslate nohighlight">\( Y \)</span> represent the total number of birthday matches, which can be written as the sum of indicator variables <span class="math notranslate nohighlight">\( I_j \)</span>, where <span class="math notranslate nohighlight">\( I_j = 1 \)</span> if the <span class="math notranslate nohighlight">\( j \)</span>-th pair of individuals share a birthday, and <span class="math notranslate nohighlight">\( I_j = 0 \)</span> otherwise. Thus, we can express <span class="math notranslate nohighlight">\( Y \)</span> as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
Y = I_1 + I_2 + \cdots + I_{\binom{n}{2}}.
\end{equation*}\]</div>
<p><strong>Step 2: Define the goal.</strong> We aim to compute <span class="math notranslate nohighlight">\( \mathbb{E}[Y] \)</span>, the expected number of birthday matches. Using the linearity of expectation, we can break this sum down into the individual expectations for each pair:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
		\mathbb{E}[Y] &amp;= \mathbb{E}\left[I_1 + I_2 + \cdots + I_{{n \choose 2}}\right] = \mathbb{E}\left[I_1\right] + \mathbb{E}\left[I_2\right] + \cdots + \mathbb{E}\left[I_{{n \choose 2}}\right]\\
		&amp;= \sum_{j = 1}^{{n \choose 2}} \mathbb{P}[\text{$j^{th}$ pair share birthday}].
\end{align*}\]</div>
<p><strong>Step 3: Calculate the probability for each pair.</strong> For any pair of people, the probability that they share the same birthday can be computed as follows. The probability that the first person in the pair is born on any specific day (e.g., January 1st) is <span class="math notranslate nohighlight">\( \frac{1}{365} \)</span>, and the probability that the second person is also born on that same day is also <span class="math notranslate nohighlight">\( \frac{1}{365} \)</span>. Therefore, the probability that both individuals in a pair are born on the same day is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
			\mathbb{P}[\text{pair share b-day}] &amp;= \mathbb{P}[\text{pair born on Jan 1 or pair born on Jan 2 or }\dots]\\
			&amp;= \mathbb{P}[\text{pair born on Jan 1}] + \mathbb{P}[\text{pair born on Jan 2}] + \cdots\\
			&amp;= \sum_{i = 1}^{365}\mathbb{P}[\text{pair both born on day }i]\\
      &amp;=  \sum_{i = 1}^{365}\frac{1}{365} \cdot \frac{1}{365}\\
			&amp;= \frac{1}{365}.
\end{align*}\]</div>
<p><strong>Step 4: Compute the final answer.</strong> Using this probability, we can now compute the expected number of birthday matches. Since there are <span class="math notranslate nohighlight">\( \binom{n}{2} \)</span> pairs in the room, the expected number of matches is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{E}[Y] = \frac{\binom{n}{2}}{365}.
\end{equation*}\]</div>
<p>For example, in a room with 70 people, the expected number of birthday matches is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{E}[\# \text{ birthday matches}] = \frac{\binom{70}{2}}{365} \approx 6.6.
\end{equation*}\]</div>
</div>
</section>
<section id="law-of-the-unconscious-statistician">
<h2><a class="toc-backref" href="#id9" role="doc-backlink"><span class="section-number">4.5. </span>Law of the unconscious statistician</a><a class="headerlink" href="#law-of-the-unconscious-statistician" title="Link to this heading">#</a></h2>
<p>The <em>law of the unconscious statistician (LOTUS)</em> is a simple formula for calculating the expected value of functions of RVs, namely <span class="math notranslate nohighlight">\(\mathbb{E}[g(X)]\)</span> where <span class="math notranslate nohighlight">\(X\)</span> is a RV and <span class="math notranslate nohighlight">\(g\)</span> is any function.</p>
<div class="admonition-law-of-the-unconcious-statistician admonition">
<p class="admonition-title">Law of the unconcious statistician</p>
<p>For any function <span class="math notranslate nohighlight">\(g: \mathbb{R} \to \mathbb{R}\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[g(X)] = \sum_x g(x)\mathbb{P}[X = x]\)</span>.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Example: St. Petersburg paradox</p>
<p>This is Example 4.3.14 from <span id="id3">[<a class="reference internal" href="bibliography.html#id2" title="Joseph K Blitzstein and Jessica Hwang. Introduction to probability. Chapman and Hall/CRC, 2019.">BH19</a>]</span>.</p>
<p>Suppose you play a game where you flip a fair coin repeatedly until it lands heads. The payout structure is as follows: if the 1st heads appears on the 1st round, you win $2; if it appears on the 2nd round, you win $4; if it appears on the <span class="math notranslate nohighlight">\(i\)</span>-th round, you win <span class="math notranslate nohighlight">\(2^i\)</span> dollars.</p>
<p><strong>Question:</strong> Let <span class="math notranslate nohighlight">\( X \)</span> represent your total winnings from the game. What is the expected value <span class="math notranslate nohighlight">\( \mathbb{E}[X] \)</span>?</p>
<p><strong>Step 1: Define the random variables.</strong> Let <span class="math notranslate nohighlight">\( N \)</span> denote the number of rounds it takes to get the first heads. Since the winnings are determined by the round on which the first heads occurs, we can express <span class="math notranslate nohighlight">\( X \)</span> as a function of <span class="math notranslate nohighlight">\( N \)</span>: <span class="math notranslate nohighlight">\(X = 2^N = g(N).\)</span></p>
<p><strong>Step 2: Define the goal.</strong> We want to calculate the expected value of <span class="math notranslate nohighlight">\( X \)</span>. Using the fact that <span class="math notranslate nohighlight">\( X = g(N) \)</span>, we compute <span class="math notranslate nohighlight">\( \mathbb{E}[X] \)</span> as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{E}[X] = \mathbb{E}[g(N)] = \sum_{n=1}^{\infty} g(n) \cdot \mathbb{P}[N = n].
\end{equation*}\]</div>
<p><strong>Step 3: Is <span class="math notranslate nohighlight">\(N\)</span> a familiar RV?</strong> The random variable <span class="math notranslate nohighlight">\( N \)</span> follows the first success distribution because it counts the number of rounds until the first heads appears in a sequence of fair coin flips, including that first head. Specifically, <span class="math notranslate nohighlight">\( N \sim \text{FS}(\frac{1}{2}) \)</span>, which means the probability that the first heads occurs on the <span class="math notranslate nohighlight">\(n\)</span>-th round is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{P}[N = n] = \frac{1}{2^n}.
\end{equation*}\]</div>
<p><strong>Step 4: Compute the expected value.</strong> Substituting the expression for <span class="math notranslate nohighlight">\( g(n) = 2^n \)</span> and the probability <span class="math notranslate nohighlight">\( \mathbb{P}[N = n] = \frac{1}{2^n} \)</span> into the expected value formula, we get:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{E}[X] = \sum_{n=1}^{\infty} 2^n \cdot \frac{1}{2^n} = 2 \cdot \frac{1}{2} + 4 \cdot \frac{1}{4} + 8 \cdot \frac{1}{8} + \cdots.
\end{equation*}\]</div>
<p>Each term simplifies to 1, and since there are infinitely many terms, the sum diverges: <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \infty.\)</span> Thus, the expected winnings in the St. Petersburg game are infinite.</p>
<p>This result is counterintuitive because it suggests that a rational person should be willing to pay any amount to play this game, despite the unrealistic nature of achieving such high winnings. In fact, since <span class="math notranslate nohighlight">\(N \sim \text{FS}\left(\frac{1}{2}\right)\)</span>, so the expected number of rounds of the game is <span class="math notranslate nohighlight">\(\mathbb{E}[N] = 2\)</span>.</p>
<p>It’s important to note that <span class="math notranslate nohighlight">\( \infty = \mathbb{E}[X] = \mathbb{E}\left[2^N\right] \not=  2^{\mathbb{E}[N]} = 4\)</span>.</p>
<p>To make the game more realistic, suppose you impose a bound of 40 rounds. In this case, the maximum potential winnings are $<span class="math notranslate nohighlight">\( 2^{40} \)</span>, which is approximately one trillion dollars. However, the expected value of <span class="math notranslate nohighlight">\( X \)</span> when limiting the number of rounds to 40 can be calculated as <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \sum_{n = 1}^{40} \frac{1}{2^n} \cdot 2^n = 40.\)</span> By capping the game at 40 rounds, the expected winnings become finite and are equal to 40 dollars.</p>
</div>
</section>
<section id="variance">
<h2><a class="toc-backref" href="#id10" role="doc-backlink"><span class="section-number">4.6. </span>Variance</a><a class="headerlink" href="#variance" title="Link to this heading">#</a></h2>
<p>Variance is a measure that describes how “spread out” a probability distribution is. It quantifies the average squared deviation of a random variable <span class="math notranslate nohighlight">\( X \)</span> from its mean.</p>
<div class="admonition-variance admonition">
<p class="admonition-title">Variance</p>
<p>The variance of a RV <span class="math notranslate nohighlight">\(X\)</span> is defined as <span class="math notranslate nohighlight">\(\text{Var}(X) = \mathbb{E}\left[(X - \mathbb{E}[X])^2\right].\)</span></p>
</div>
<section id="why-variance-is-defined-this-way">
<h3><a class="toc-backref" href="#id11" role="doc-backlink"><span class="section-number">4.6.1. </span>Why variance is defined this way</a><a class="headerlink" href="#why-variance-is-defined-this-way" title="Link to this heading">#</a></h3>
<p>A natural first attempt to measure spread might be to compute the expectation of the difference between the random variable and its mean, i.e., <span class="math notranslate nohighlight">\( \mathbb{E}[X - \mathbb{E}[X]] \)</span>. However, this turns out to be zero by linearity of expectation: <span class="math notranslate nohighlight">\(\mathbb{E}[X - \mathbb{E}[X]] = 0.\)</span> Next, one might try using the absolute deviation, <span class="math notranslate nohighlight">\( \mathbb{E}[|X - \mathbb{E}[X]|] \)</span>, but this function is not differentiable, which makes it more difficult to work with mathematically. The definition of variance as <span class="math notranslate nohighlight">\(\mathbb{E}\left[(X - \mathbb{E}[X])^2\right]\)</span> both captures the spread of a distribution and is easy to work with mathematically.</p>
</section>
<section id="standard-deviation">
<h3><a class="toc-backref" href="#id12" role="doc-backlink"><span class="section-number">4.6.2. </span>Standard deviation</a><a class="headerlink" href="#standard-deviation" title="Link to this heading">#</a></h3>
<p>It’s important to note that when a RV is measured in terms of some unit (say, inches), then its variance will be measured in terms of that unit squared (say, inches^2). To express variance in the same units as the original random variable, we take the square root of the variance, yielding the standard deviation.</p>
<div class="admonition-standard-deviation admonition">
<p class="admonition-title">Standard deviation</p>
<p>The standard deviation of a RV <span class="math notranslate nohighlight">\(X\)</span> is defined as <span class="math notranslate nohighlight">\(\text{SD}(X) = \sqrt{\text{Var}(X)}.\)</span></p>
</div>
</section>
<section id="alternate-expression-for-variance">
<h3><a class="toc-backref" href="#id13" role="doc-backlink"><span class="section-number">4.6.3. </span>Alternate expression for variance</a><a class="headerlink" href="#alternate-expression-for-variance" title="Link to this heading">#</a></h3>
<p>Another way to express variance simplifies the computation by expanding the squared term:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\text{Var}(X) = \mathbb{E}[X^2 + 2X\mathbb{E}[X] - \mathbb{E}[X]^2] = \mathbb{E}[X^2] - 2\mathbb{E}[X]^2 + \mathbb{E}[X]^2 = \mathbb{E}[X^2] - \mathbb{E}[X]^2.
\end{equation*}\]</div>
<p>This version of the formula shows that variance is the difference between the expected value of <span class="math notranslate nohighlight">\( X^2 \)</span> and the square of the expected value of <span class="math notranslate nohighlight">\( X \)</span>.</p>
</section>
<section id="important-properties-of-variance">
<h3><a class="toc-backref" href="#id14" role="doc-backlink"><span class="section-number">4.6.4. </span>Important properties of variance</a><a class="headerlink" href="#important-properties-of-variance" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Shifting by a constant</strong>: Adding a constant <span class="math notranslate nohighlight">\( c \)</span> to <span class="math notranslate nohighlight">\( X \)</span> does not change its variance: <span class="math notranslate nohighlight">\(\text{Var}(X + c) = \text{Var}(X).\)</span></p></li>
<li><p><strong>Scaling by a constant</strong>: Multiplying <span class="math notranslate nohighlight">\( X \)</span> by a constant <span class="math notranslate nohighlight">\( c \)</span> scales the variance by <span class="math notranslate nohighlight">\( c^2 \)</span>: <span class="math notranslate nohighlight">\(\text{Var}(cX) = c^2 \text{Var}(X).\)</span></p></li>
<li><p><strong>Independence</strong>: If <span class="math notranslate nohighlight">\( X \)</span> and <span class="math notranslate nohighlight">\( Y \)</span> are independent, the variance of their sum is the sum of their variances: <span class="math notranslate nohighlight">\(\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y).\)</span></p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Example: Variance of a binomial random variable</p>
<p>This is Example 4.6.5 from <span id="id4">[<a class="reference internal" href="bibliography.html#id2" title="Joseph K Blitzstein and Jessica Hwang. Introduction to probability. Chapman and Hall/CRC, 2019.">BH19</a>]</span>.</p>
<p>Recall that the if <span class="math notranslate nohighlight">\( X \sim \text{Bin}(n, p) \)</span>, we can express <span class="math notranslate nohighlight">\( X \)</span> as the sum of <span class="math notranslate nohighlight">\( n \)</span> independent Bernoulli variables <span class="math notranslate nohighlight">\( I_1, I_2, \dots, I_n \sim \text{Bern}(p) \)</span>: <span class="math notranslate nohighlight">\(X = I_1 + I_2 + \cdots + I_n.\)</span> For each <span class="math notranslate nohighlight">\( I_j \)</span>, the expected value is <span class="math notranslate nohighlight">\( \mathbb{E}[I_j] = p \)</span>, and since <span class="math notranslate nohighlight">\( I_j^2 = I_j \)</span>, we have that <span class="math notranslate nohighlight">\(\mathbb{E}[I_j^2] = \mathbb{E}[I_j] = p.\)</span> The variance of each <span class="math notranslate nohighlight">\( I_j \)</span> is then <span class="math notranslate nohighlight">\(\text{Var}(I_j) = \mathbb{E}[I_j^2] - \mathbb{E}[I_j]^2 = p - p^2 = p(1 - p).\)</span> Since <span class="math notranslate nohighlight">\( X \)</span> is the sum of <span class="math notranslate nohighlight">\( n \)</span> independent indicators, the variance of <span class="math notranslate nohighlight">\( X \)</span> is: <span class="math notranslate nohighlight">\(\text{Var}(X) = \text{Var}(I_1) + \text{Var}(I_2) + \cdots + \text{Var}(I_n) = np(1 - p).\)</span></p>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="chapter3.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Random variables and their distributions</p>
      </div>
    </a>
    <a class="right-next"
       href="bibliography.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Bibliography</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-of-expectation">4.1. Definition of expectation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linearity-of-expectation">4.2. Linearity of Expectation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#geometric-and-negative-binomial">4.3. Geometric and negative binomial</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#indicator-random-variables-and-the-fundamental-bridge">4.4. Indicator random variables and the fundamental bridge</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#law-of-the-unconscious-statistician">4.5. Law of the unconscious statistician</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">4.6. Variance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-variance-is-defined-this-way">4.6.1. Why variance is defined this way</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-deviation">4.6.2. Standard deviation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alternate-expression-for-variance">4.6.3. Alternate expression for variance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-properties-of-variance">4.6.4. Important properties of variance</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Ellen Vitercik
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>